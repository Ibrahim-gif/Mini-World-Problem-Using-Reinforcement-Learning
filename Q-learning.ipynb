{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c29949",
   "metadata": {},
   "source": [
    "<h3> Q Learning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e440cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f20388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the shape of the mini-world\n",
    "rows = 4\n",
    "columns = 6\n",
    "\n",
    "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
    "#The array has 4 rows and 6 columns and each cell is a list of actions defined below\n",
    "q_values = np.zeros((rows, columns, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02350b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42b1711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial set of Rewards before training the model\n",
      "╒═══╤═══╤═══╤═══╤════╤════╕\n",
      "│ 0 │ 0 │ 0 │ 0 │  0 │  0 │\n",
      "├───┼───┼───┼───┼────┼────┤\n",
      "│ 0 │ 0 │ 0 │ 0 │  0 │  1 │\n",
      "├───┼───┼───┼───┼────┼────┤\n",
      "│ 0 │ 0 │ 0 │ 0 │  0 │  0 │\n",
      "├───┼───┼───┼───┼────┼────┤\n",
      "│ 0 │ 0 │ 0 │ 0 │ -1 │ -1 │\n",
      "╘═══╧═══╧═══╧═══╧════╧════╛\n"
     ]
    }
   ],
   "source": [
    "#Created a 2D numpy array to hold the rewards for each state. \n",
    "#The array contains 4 rows and 6 columns \n",
    "# each value is initialized to 0.\n",
    "rewards = np.full((rows, columns), 0)\n",
    "\n",
    "\n",
    "#set the rewards for all aisle locations (i.e., white squares)\n",
    "for i in range(rows):\n",
    "  for j in range(columns):\n",
    "    rewards[i, j] = 0\n",
    "  \n",
    "#Set the Loss states (4,5) and (4,6)\n",
    "rewards[3,4] = -1\n",
    "rewards[3,5] = -1\n",
    "\n",
    "#Set the win state (2,6) \n",
    "rewards[1, 5] = 1 \n",
    "\n",
    "print(\"Initial set of Rewards before training the model\")\n",
    "print(tabulate(rewards, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e7a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that returns true or flase if a state is terminal state\n",
    "def is_terminal_state(row_index, column_index):\n",
    "  #if the reward for this location is 0, then it is not a terminal state, hence return false\n",
    "  if rewards[row_index, column_index] == 0:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "#define an epsilon greedy algorithm that will choose which action to take next\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "  #if a randomly chosen value between 0 and 1 is less than epsilon, then choose the most promising value from the Q-table for this state.\n",
    "  if np.random.random() < epsilon:\n",
    "    return np.argmax(q_values[current_row_index, current_column_index])\n",
    "  else: #choose a random action\n",
    "    return np.random.randint(4)\n",
    "\n",
    "#define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "  new_row_index = current_row_index\n",
    "  new_column_index = current_column_index\n",
    "  if actions[action_index] == 'up' and current_row_index > 0:\n",
    "    new_row_index -= 1\n",
    "  elif actions[action_index] == 'right' and current_column_index < columns - 1:\n",
    "    new_column_index += 1\n",
    "  elif actions[action_index] == 'down' and current_row_index < rows - 1:\n",
    "    new_row_index += 1\n",
    "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "    new_column_index -= 1\n",
    "  return new_row_index, new_column_index\n",
    "\n",
    "#define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "  #get a random row and column index\n",
    "  row_index = np.random.randint(rows)\n",
    "  column_index = np.random.randint(columns)\n",
    "  return row_index, column_index\n",
    "\n",
    "\n",
    "#Function that calcultes the shortest path\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  # Base case as return immediately if this is an invalid starting location\n",
    "  if is_terminal_state(start_row_index, start_column_index):\n",
    "    return []\n",
    "  else: #if valid starting location\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "      #get the best action to take\n",
    "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "      #move to the next location on the path, and add the new location to the list\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "142fe1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning complete!\n"
     ]
    }
   ],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.8 #the probability that it will take a best action instead of a random action \n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "\n",
    "#The problem statement states to attemp for atleast 20 training paths, so lets train the model for 100 \n",
    "for episode in range(1000):\n",
    "  #get the starting location for this episode\n",
    "  row_index, column_index = get_starting_location()\n",
    "\n",
    "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "  while not is_terminal_state(row_index, column_index):\n",
    "    #choose which action to take (i.e., where to move next)\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "    \n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[row_index, column_index]\n",
    "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    \n",
    "    #(R(s) + γ max Q(s0, a0) − Q(s, a))\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "    #Q(s, a) <- Q(s, a) + temporal_difference\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Q Learning complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a6049e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 5]]\n"
     ]
    }
   ],
   "source": [
    "print(get_shortest_path(0, 0)) #starting at row 0, column 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b71b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1], [2, 2], [2, 3], [1, 3], [1, 4], [1, 5]]\n"
     ]
    }
   ],
   "source": [
    "print(get_shortest_path(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a979fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6, 4)\n"
     ]
    }
   ],
   "source": [
    "print(q_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbcfaf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Utility Matrix is:\n",
      "\n",
      "[0.53, 0.59, 0.59, 0.53]  |  [0.59, 0.66, 0.66, 0.53]  |  [0.66, 0.73, 0.73, 0.59]  |  [0.73, 0.81, 0.81, 0.66]  |  [0.81, 0.9, 0.9, 0.73]  |  [0.9, 0.9, 1.0, 0.81]  |  \n",
      "\n",
      "[0.53, 0.66, 0.53, 0.48]  |  [0.59, 0.73, 0.59, 0.59]  |  [0.66, 0.81, 0.66, 0.66]  |  [0.73, 0.9, 0.73, 0.73]  |  [0.81, 1.0, 0.81, 0.81]  |  [0.0, 0.0, 0.0, 0.0]  |  \n",
      "\n",
      "[0.59, 0.59, 0.0, 0.53]  |  [0.66, 0.66, 0.53, 0.53]  |  [0.73, 0.73, 0.59, 0.59]  |  [0.81, 0.81, 0.66, 0.66]  |  [0.9, 0.9, -1.0, 0.73]  |  [1.0, 0.9, -1.0, 0.81]  |  \n",
      "\n",
      "[0.52, 0.53, 0.48, 0.0]  |  [0.59, 0.53, 0.53, 0.48]  |  [0.66, 0.63, 0.58, 0.0]  |  [0.73, -0.9, 0.66, 0.59]  |  [0.0, 0.0, 0.0, 0.0]  |  [0.0, 0.0, 0.0, 0.0]  |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The Utility Matrix is:\")\n",
    "print()\n",
    "for i in range(4):\n",
    "    for j in range(6):\n",
    "        new_list = [round(item, 2) for item in q_values[i][j]]\n",
    "        print( new_list, \" | \", end=\" \")\n",
    "        \n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2903a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(6):\n",
    "        max = q_values[i][j][0]\n",
    "        index = 0\n",
    "        for k in range(4):\n",
    "            if q_values[i][j][k] > max:\n",
    "                max = q_values[i][j][k]\n",
    "                index = k\n",
    "                \n",
    "                \n",
    "        arr.append(index)\n",
    "\n",
    "arrows_matrix = []\n",
    "\n",
    "for i in arr:\n",
    "    if(i == 0):\n",
    "        arrows_matrix.append(\"↑\")\n",
    "    elif (i == 1):\n",
    "        arrows_matrix.append(\"→\")\n",
    "    elif(i == 2):\n",
    "        arrows_matrix.append(\"↓\")\n",
    "    elif(i == 3):\n",
    "        arrows_matrix.append(\"←\")\n",
    "\n",
    "arrows_matrix[11] = 1\n",
    "arrows_matrix[23] = -1\n",
    "arrows_matrix[22] = -1\n",
    "\n",
    "np_arrow = np.array(arrows_matrix).reshape(4,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b28e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤═════╤═════╤═════╤═════╤═════╕\n",
      "│ P   │ o   │ l   │ i   │ c   │ y   │\n",
      "╞═════╪═════╪═════╪═════╪═════╪═════╡\n",
      "│ →   │ →   │ →   │ →   │ →   │ ↓   │\n",
      "├─────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│ →   │ →   │ →   │ →   │ →   │ 1   │\n",
      "├─────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│ →   │ →   │ →   │ ↑   │ →   │ ↑   │\n",
      "├─────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│ →   │ ↑   │ ↑   │ ↑   │ -1  │ -1  │\n",
      "╘═════╧═════╧═════╧═════╧═════╧═════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(np_arrow, headers='Policy', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811c1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
